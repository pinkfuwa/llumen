---
title: "Model Configuration"
description: "Advanced model settings and capabilities"
---

## Overview

Llumen allows you to configure individual models with specific capabilities and parameters. This enables fine-tuned control over how each AI model behaves.

## Configuration Format

Model configurations are stored in TOML format. Each model has:
- **Display name** - Human-readable name shown in UI
- **Model ID** - Identifier used by the API provider
- **Capabilities** - What the model supports(optional)
- **Parameters** - Inference settings(optional)

## Basic Configuration

### Minimal Example

```toml
display_name = "GPT-OSS 20B"
# openrouter suffix are supported
model_id = "openai/gpt-oss-20b:nitro"
```

### Complete Example

```toml
display_name = "Claude 4.5 Sonnet"
model_id = "anthropic/claude-4.5-sonnet"

[capability]
image = true
audio = false
ocr = "Mistral"
tool = true
json = true
reasoning = true

[parameter]
temperature = 0.7
top_p = 0.9
top_k = 40
repeat_penalty = 1.1
```

## Model Capabilities

Configure what features the model supports:

<Note>
You don't need this if you are using openrouter. Openrouter API allow llumen to detect it for you.
</Note>

### Image Input Support

<Note>
Image output are openrouter only. And you don't need to specify it in config.
</Note>

```toml
[capability]
image = true  # Model can take images
```

### Audio Input Support

```toml
[capability]
audio = true  # Model can take audio
```

### OCR Engine

```toml
[capability]
ocr = "Mistral"  # Options: "Native", "Text", "Mistral", "Disabled"
```

**OCR Engines:**

<Accordion title="native">
  Built-in OCR processing using local libraries.
  
  **Pros:**
  - Fast
  - Can extract image
  
  **Cons:**
  - Limited provider support
</Accordion>

<Accordion title="text">
  Basic text extraction from PDFs.
  
  **Best for:**
  - Text-based PDFs
  - Simple documents
</Accordion>

<Accordion title="mistral">
  Uses Mistral's vision model for OCR.
  
  **Pros:**
  - High accuracy
  - Multiple languages
  - Handles complex layouts
  
  **Cons:**
  - Slower
  - Additional cost
</Accordion>

### Tool Use

```toml
[capability]
tool = true  # Model can use tools/function calling
```

**Required for:**
- Search mode
- Research mode

### JSON Mode

```toml
[capability]
json = true  # Model supports structured output
```

**When enabled:**
- More reliable research mode

### Reasoning

```toml
[capability]
reasoning = true  # Model has enhanced reasoning
```

**For models like:**
- o1-preview
- o1-mini

**Enables:**
- Step-by-step thinking
- interleaved thinking(If model support it)
- Complex problem solving

## Model Parameters

Fine-tune inference behavior:

### Temperature

```toml
[parameter]
temperature = 0.7  # Range: 0.0 - 2.0
```

Controls randomness in responses:

| Value | Behavior | Best For |
|-------|----------|----------|
| 0.0 - 0.3 | Deterministic, focused | Code, facts, consistency |
| 0.4 - 0.7 | Balanced | General chat, Q&A |
| 0.8 - 1.0 | Creative | Writing, brainstorming |
| 1.1+ | Very random | Experimental, creative |

**Examples:**

```toml
# For coding assistance
temperature = 0.2

# For creative writing
temperature = 0.9

# For balanced chat
temperature = 0.7
```

### Top P (Nucleus Sampling)

```toml
[parameter]
top_p = 0.9  # Range: 0.0 - 1.0
```

Controls diversity by limiting token selection:

- **0.1** - Very focused, predictable
- **0.5** - Moderate diversity
- **0.9** - High diversity (recommended)
- **1.0** - All tokens considered

<Note>
  Use either `temperature` OR `top_p`, not both. OpenRouter recommends `top_p`.
</Note>

### Top K

```toml
[parameter]
top_k = 40  # Range: 1 - 100+
```

Limits selection to top K most probable tokens:

- **10-20** - Conservative, focused
- **40-50** - Balanced (recommended)
- **100+** - More diverse

### Repeat Penalty

```toml
[parameter]
repeat_penalty = 1.1  # Range: 1.0 - 2.0
```

Reduces repetition in responses:

- **1.0** - No penalty (repetitive)
- **1.1** - Light penalty (recommended)
- **1.2-1.3** - Moderate penalty
- **1.5+** - Strong penalty (may affect quality)

## Configuring Models in Llumen

### Via Web Interface

1. Log in to llumen
2. Go to **Settings** â†’ **Openrouter**
3. Add or edit model configurations
4. Save changes

## Next Steps

<CardGroup cols={2}>
  <Card title="API Providers" href="/user-guide/api-providers">
    Configure different AI providers
  </Card>
  <Card title="Configuration" href="/user-guide/configuration">
    General llumen configuration
  </Card>
  <Card title="Chat Modes" href="/features/chat-modes">
    Learn about different chat modes
  </Card>
  <Card title="Troubleshooting" href="/user-guide/troubleshooting">
    Fix common issues
  </Card>
</CardGroup>
