---
title: "API Providers"
description: "Configure language model providers"
---

## OpenRouter (Recommended)

**Why OpenRouter:**
- Access to 100+ models
- Pay-per-use, no subscriptions
- Automatic fallbacks
- Competitive pricing

**Setup:**

1. Sign up at [openrouter.ai](https://openrouter.ai)
2. Generate API key
3. Set in llumen:

```bash
export API_KEY="sk-or-v1-your-key"
# API_BASE defaults to OpenRouter
```

**Popular Models:**
- `anthropic/claude-3.5-sonnet` - Best overall
- `openai/gpt-4-turbo` - Fast and capable
- `meta-llama/llama-3-70b` - Open source
- `google/gemini-pro` - Google's model

## OpenAI

**Setup:**

```bash
export API_KEY="sk-your-openai-key"
export API_BASE="https://api.openai.com"
```

**Models:**
- `gpt-4-turbo` - Most capable
- `gpt-4` - Balanced
- `gpt-3.5-turbo` - Fast and cheap

## Local Models (Ollama)

**Setup:**

1. Install [Ollama](https://ollama.ai)
2. Pull models:
   ```bash
   ollama pull llama3
   ollama pull mistral
   ```
3. Configure llumen:
   ```bash
   export API_KEY="ollama"
   export API_BASE="http://localhost:11434/v1"
   ```

**Benefits:**
- 100% private
- No API costs
- Works offline

**Tradeoffs:**
- Slower responses
- Requires powerful hardware
- Lower quality than GPT-4/Claude

## Other Providers

Any OpenAI-compatible API works:

```bash
export API_KEY="your-key"
export API_BASE="https://your-provider.com/v1"
```

**Compatible services:**
- Together AI
- Anyscale
- Replicate
- Groq
- LM Studio
- LocalAI

## Model Selection

### In the UI

1. Open Settings
2. Go to Models
3. Select from available models
4. Save preferences

### For Different Tasks

**Fast responses:** GPT-3.5, Mistral
**Best quality:** GPT-4, Claude 3.5 Sonnet
**Privacy:** Local models via Ollama
**Cost-effective:** Llama 3 via OpenRouter

## Rate Limits

Different providers have different limits:
- OpenRouter: Pay-per-use, generous limits
- OpenAI: Tiered by subscription
- Local: Limited by hardware

## Next Steps

<CardGroup cols={2}>
  <Card title="Configuration" href="/user-guide/configuration">
    Detailed configuration options
  </Card>
  <Card title="Docker Compose" href="/user-guide/docker-compose">
    Production deployment
  </Card>
</CardGroup>
