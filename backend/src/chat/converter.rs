use std::sync::Arc;

use protocol::{AssistantChunk, FileMetadata, MessageInner};

use crate::{
    chat::{self, Context},
    openrouter,
    utils::blob::BlobDB,
};
use anyhow::Result;

async fn load_files(
    db: Arc<BlobDB>,
    handles: &[FileMetadata],
) -> Result<Vec<openrouter::File>, anyhow::Error> {
    let mut tasks = Vec::with_capacity(handles.len());

    for handle in handles {
        let id = handle.id;
        let name = handle.name.clone();
        let db = db.clone();
        let handle = tokio::spawn(async move {
            db.get_vectored(id)
                .await
                .map(|data| openrouter::File { name, data })
        });
        tasks.push(handle);
    }

    let mut results = Vec::with_capacity(handles.len());
    for task in tasks {
        match task.await? {
            Some(it) => results.push(it),
            None => log::error!("File not found"),
        };
    }

    Ok(results)
}

pub async fn db_message_to_openrouter(
    ctx: &Context,
    message: &MessageInner,
) -> Result<impl Iterator<Item = openrouter::Message>> {
    let mut result = Vec::new();
    match message {
        MessageInner::User { text, files } => {
            if files.is_empty() {
                result.push(openrouter::Message::User(text.clone()));
            } else {
                let file_data = load_files(ctx.blob.clone(), files).await?;
                result.push(openrouter::Message::MultipartUser {
                    text: text.clone(),
                    files: file_data,
                });
            }
        }
        MessageInner::Assistant(assistant_chunks) => {
            let mut iter = assistant_chunks.iter().peekable();
            while let Some(chunk) = iter.next() {
                match chunk {
                    AssistantChunk::Annotation(_) => {
                        #[cfg(debug_assertions)]
                        panic!("Annotation should be captured by Text chunk");
                    }
                    AssistantChunk::Text(x) => {
                        if matches!(iter.peek(), Some(AssistantChunk::Annotation(_))) {
                            let annotation = iter.next().unwrap().as_annotation().unwrap();
                            result.push(openrouter::Message::AssistantAnnotationed {
                                text: x.clone(),
                                annotations: serde_json::to_value(annotation)
                                    .unwrap_or(serde_json::Value::Null),
                            });
                        } else {
                            result.push(openrouter::Message::Assistant(x.clone()))
                        }
                    }
                    AssistantChunk::Reasoning(_) => {
                        // Reasoning is internal and not sent back to the model
                    }
                    AssistantChunk::ToolCall { id, arg, name } => {
                        result.push(openrouter::Message::ToolCall(openrouter::MessageToolCall {
                            id: id.clone(),
                            name: name.clone(),
                            arguments: arg.clone(),
                        }));
                    }
                    AssistantChunk::ToolResult { id, response } => {
                        result.push(openrouter::Message::ToolResult(
                            openrouter::MessageToolResult {
                                id: id.clone(),
                                content: response.clone(),
                            },
                        ));
                    }
                    AssistantChunk::Error(_) => {
                        // Errors are not sent to the model
                    }
                    AssistantChunk::DeepAgent(_deep) => {
                        // DeepAgent is internal state and not sent to the model
                        // report generated by deep research is another text chunk
                    }
                }
            }
        }
    };
    Ok(result.into_iter())
}

pub fn openrouter_stream_to_assitant_chunk(
    msgs: &[openrouter::StreamCompletionResp],
) -> impl Iterator<Item = AssistantChunk> {
    let mut result = Vec::new();
    for msg in msgs {
        match msg {
            openrouter::StreamCompletionResp::ReasoningToken(x) => {
                if let Some(AssistantChunk::Reasoning(reasoning)) = result.last_mut() {
                    reasoning.push_str(x.as_str());
                } else {
                    result.push(AssistantChunk::Reasoning(x.clone()));
                }
            }
            openrouter::StreamCompletionResp::ResponseToken(x) => {
                if let Some(AssistantChunk::Text(response)) = result.last_mut() {
                    response.push_str(x.as_str());
                } else {
                    result.push(AssistantChunk::Text(x.clone()));
                }
            }
            openrouter::StreamCompletionResp::Usage { .. } => {}
            openrouter::StreamCompletionResp::ToolToken { .. } => {
                // ToolToken is for streaming chunks, typically merged into the final ToolCall
                // For now, we can skip them as they're intermediate states
            }
        }
    }
    result.into_iter()
}

pub fn openrouter_to_buffer_token(token: openrouter::StreamCompletionResp) -> chat::Token {
    match token {
        openrouter::StreamCompletionResp::ResponseToken(content) => chat::Token::Assistant(content),
        openrouter::StreamCompletionResp::ReasoningToken(content) => {
            chat::Token::Reasoning(content)
        }
        openrouter::StreamCompletionResp::ToolToken { .. } => {
            // ToolToken is intermediate state during streaming, return empty
            chat::Token::Empty
        }
        openrouter::StreamCompletionResp::Usage { .. } => {
            // Usage info is handled separately
            chat::Token::Empty
        }
    }
}

pub fn openrouter_to_buffer_token_deep_plan(
    token: openrouter::StreamCompletionResp,
) -> chat::Token {
    match token {
        openrouter::StreamCompletionResp::ResponseToken(content) => chat::Token::DeepPlan(content),
        openrouter::StreamCompletionResp::ReasoningToken(_) => chat::Token::Empty,
        openrouter::StreamCompletionResp::ToolToken { .. } => chat::Token::Empty,
        openrouter::StreamCompletionResp::Usage { .. } => chat::Token::Empty,
    }
}

pub fn openrouter_to_buffer_token_deep_step(
    token: openrouter::StreamCompletionResp,
) -> chat::Token {
    match token {
        openrouter::StreamCompletionResp::ResponseToken(content) => {
            chat::Token::DeepStepToken(content)
        }
        openrouter::StreamCompletionResp::ReasoningToken(content) => {
            chat::Token::DeepStepReasoning(content)
        }
        openrouter::StreamCompletionResp::ToolToken { .. } => chat::Token::Empty,
        openrouter::StreamCompletionResp::Usage { .. } => chat::Token::Empty,
    }
}

pub fn openrouter_to_buffer_token_deep_report(
    token: openrouter::StreamCompletionResp,
) -> chat::Token {
    match token {
        openrouter::StreamCompletionResp::ResponseToken(content) => {
            chat::Token::DeepReport(content)
        }
        openrouter::StreamCompletionResp::ReasoningToken(content) => {
            chat::Token::Reasoning(content)
        }
        openrouter::StreamCompletionResp::ToolToken { .. } => chat::Token::Empty,
        openrouter::StreamCompletionResp::Usage { .. } => chat::Token::Empty,
    }
}
